{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5284ede",
   "metadata": {},
   "source": [
    "# LLMRouterChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3fd50f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os \n",
    "import getpass\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chains import LLMChain,SequentialChain\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1da30bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc1fc9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcc020f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bharanidharan.a\\AppData\\Local\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:151: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15220cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_template=\"\"\"You are a very smart human resources person. \\\n",
    "You are great at answering questions about human resources in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37f421da",
   "metadata": {},
   "outputs": [],
   "source": [
    "law_template=\"\"\"You are a very smart lawyer. \\\n",
    "You are great at answering questions about the law in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ca50521",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "{\n",
    "    'name':'human resources',\n",
    "    'description':'Answer human resources questions',\n",
    "    'template':hr_template},\n",
    "\n",
    "{\n",
    "    'name':'law',\n",
    "    'description':'Answers legal questions',\n",
    "    'template':law_template}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9131fefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d4bec47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bharanidharan.a\\AppData\\Local\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:151: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "for p_info in prompt_infos:\n",
    "    name = p_info['name']\n",
    "    prompt_template = p_info['template']\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm,prompt=prompt)\n",
    "    destination_chains[name] = chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7b7fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template('{input}')\n",
    "\n",
    "default_chain = LLMChain(llm=llm,prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ae2dc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "```json\n",
      "{{{{\n",
      "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}}}\n",
      "```\n",
      "\n",
      "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n",
      "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "{destinations}\n",
      "\n",
      "<< INPUT >>\n",
      "{{input}}\n",
      "\n",
      "<< OUTPUT (must include ```json at the start of the response) >>\n",
      "<< OUTPUT (must end with ```) >>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(MULTI_PROMPT_ROUTER_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d266aab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "destinations=[\n",
    "f\"{p['name']}: {p['description']}\" for p in prompt_infos\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b12999c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['human resources: Answer human resources questions',\n",
       " 'law: Answers legal questions']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfe483ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human resources: Answer human resources questions\n",
      "law: Answers legal questions\n"
     ]
    }
   ],
   "source": [
    "destination_str = \"\\n\".join(destinations)\n",
    "\n",
    "print(destination_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a597cda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\\n\\n<< FORMATTING >>\\nReturn a markdown code snippet with a JSON object formatted to look like:\\n```json\\n{{\\n    \"destination\": string \\\\ name of the prompt to use or \"DEFAULT\"\\n    \"next_inputs\": string \\\\ a potentially modified version of the original input\\n}}\\n```\\n\\nREMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\\nREMEMBER: \"next_inputs\" can just be the original input if you don\\'t think any modifications are needed.\\n\\n<< CANDIDATE PROMPTS >>\\nhuman resources: Answer human resources questions\\nlaw: Answers legal questions\\n\\n<< INPUT >>\\n{input}\\n\\n<< OUTPUT (must include ```json at the start of the response) >>\\n<< OUTPUT (must end with ```) >>\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destination_str)\n",
    "\n",
    "router_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7c75f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "law: {'input': 'How do intellectual property laws address the use of AI-generated content?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Thank you for the compliment. \\n\\nIntellectual property laws are still evolving to address the use of AI-generated content. In general, copyright law protects original works of authorship, but it can be challenging to determine who the author is when content is generated by AI. Some argue that the creator of the AI should be considered the author, while others argue that the user who directs the AI should be considered the author. \\n\\nAs of now, there is no clear consensus on how to address AI-generated content under intellectual property laws. It is a complex and rapidly evolving area of law that will likely require further guidance and legislation in the future.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router_prompt = PromptTemplate(template=router_template,\n",
    "input_variables=['input'],\n",
    "output_parser=RouterOutputParser())\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm,router_prompt)\n",
    "\n",
    "chain = MultiPromptChain(router_chain=router_chain,\n",
    "destination_chains=destination_chains,\n",
    "default_chain=default_chain,verbose=True)\n",
    "\n",
    "chain.run(\"How do intellectual property laws address the use of AI-generated content?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d26c254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "human resources: {'input': 'How to effectively handle conflict between team members?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Handling conflict between team members effectively involves several key steps:\\n\\n1. Address the issue promptly: As soon as you become aware of a conflict, address it promptly to prevent it from escalating further.\\n\\n2. Encourage open communication: Encourage team members to openly communicate their concerns and perspectives on the conflict. Encourage active listening and empathetic understanding.\\n\\n3. Identify the root cause: Work with the involved parties to identify the underlying issues causing the conflict. Understanding the root cause can help in resolving the conflict effectively.\\n\\n4. Facilitate a resolution: Work with the team members involved to find a resolution that is agreeable to all parties. Encourage compromise and collaboration to find a solution that works for everyone.\\n\\n5. Follow up: After the conflict has been resolved, follow up with the team members involved to ensure that the resolution is working and that any lingering issues are addressed.\\n\\nRemember, effective conflict resolution requires patience, empathy, and a commitment to finding a solution that works for all parties involved.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"How do you handle conflict between team members?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d79918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b79ce20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
